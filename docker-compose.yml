services:
  paper-radar:
    build: .
    image: rockhhhh/paper-radar:latest
    container_name: paper-radar
    restart: unless-stopped

    environment:
      # Timezone
      - TZ=Asia/Shanghai

      # Run once on container start (useful for testing)
      - RUN_ON_START=false
      - WEB_PORT=8000

      # ===== Light LLM Configuration (for filtering) =====
      # Use any OpenAI-compatible API
      - LIGHT_LLM_API_BASE=${LIGHT_LLM_API_BASE}
      - LIGHT_LLM_API_KEY=${LIGHT_LLM_API_KEY}
      - LIGHT_LLM_MODEL=${LIGHT_LLM_MODEL}

      # ===== Heavy LLM Configuration (for PDF analysis) =====
      # Gemini via OpenAI-compatible endpoint
      - HEAVY_LLM_API_BASE=${HEAVY_LLM_API_BASE}
      - HEAVY_LLM_API_KEY=${HEAVY_LLM_API_KEY}
      - HEAVY_LLM_MODEL=${HEAVY_LLM_MODEL}

      # ===== EZproxy Configuration (for journal PDFs) =====
      - HKU_LIBRARY_UID=${HKU_LIBRARY_UID}
      - HKU_LIBRARY_PIN=${HKU_LIBRARY_PIN}

    volumes:
      # Configuration file (optional override)
      - ./config.yaml:/app/config.yaml:ro

      # Persistent storage
      - ./reports:/app/reports
      - ./logs:/app/logs
      - ./cache:/app/cache

      # Timezone sync
      - /etc/localtime:/etc/localtime:ro
    ports:
      - "8000:8000"

    # Healthcheck
    healthcheck:
      test: ["CMD", "python", "-c", "import main"]
      interval: 30s
      timeout: 10s
      retries: 3
